---
purpose: Sprint 2 data findings — rubric inflation, honest quality picture, and assumption audit patterns
last_updated: 2026-02-24
updated_by: sathya
status: active
---

# Sprint 2 Findings

Extends evidence.md with Sprint 2-specific analysis. For Sprint 1 findings and core gap definitions, see evidence.md.

---

## Critical Finding: AI-Discussion Rubric Inflation

AI-discussion submissions average **4.3/5** across both courses. Student-authored work (discovery docs, assumption audits, goal setting) averages **2.1/5**.

**Gap: 2.1-2.2 points.**

This is NOT students performing differently on different assignments — it's the Dojo prompt doing cognitive work while the rubric rewards the format:

- **Discussion sub-score**: consistently 4-5 because the AI ensures a productive conversation arc through structured steps
- **Writing sub-score**: honestly 3/5 because that's where students produce their own text
- The AI partner guides the conversation through structured steps, so transcripts look rigorous even when student thinking is shallow

### What this means

The current rubric measures "did a good conversation happen?" The AI guarantees that. It should measure "what did the student contribute to the conversation?" — their claims, their evidence, their connections.

→ See pipeline-updates.md, PIPE-7 for the rubric rework item.

---

## Honest Quality Picture (Student-Authored Work Only)

Stripping AI-discussion inflation and looking only at work where students produce their own text:

### CST395 (Sprint 2)
- 68% score 1-2/5
- 18% score 4-5/5
- **Average: 2.23**

### CST349 (Sprint 2)
- 71% score 1-2/5
- 15% score 4-5/5
- **Average: 2.04**

### Cross-Sprint Comparison

| Metric | Sprint 1 | Sprint 2 | Delta |
|---|---|---|---|
| Student-authored avg (both courses) | 2.38 | 2.11 | **-0.27** |
| CST349 student-authored avg | ~2.0 | 2.04 | Essentially flat |

Sprint 2 student-authored average (2.11) is **worse** than Sprint 1 (2.38). Partner work structure didn't lift quality — may have diluted it by adding complexity students weren't ready for. CST349 essentially flat across sprints.

---

## Per-Student Insight

Most students show this pattern:
- Student-authored average: ~1.5
- AI-discussion average: ~4.5

The gap between these two numbers is the AI's contribution, not the student's.

### Students showing independent capability above class floor

Only four students show student-authored capability meaningfully above the class floor:
- CST395-03: 3.0
- CST395-06: 2.7
- CST395-15: 3.0
- CST395-20: 3.0

**The real gap:** students aren't developing independent analytical capability — AI masks this.

---

## Assumption Audit Analysis (13 Submissions With Content)

### Pattern 1: Evidence ≠ Inference (dominant pattern)

Students say "the evidence is..." then state their interpretation.

**Example:** Partner says "I'd rather watch movies" → student concludes "he's lazy."

That's evidence the partner chooses rest sometimes. The word "lazy" is the student's, not the partner's. Preference ≠ character flaw.

Only **4 students** (files 4, 6, 10, 11) reliably separate what was said from what was interpreted.

### Pattern 2: Generic Escape-Hatch Contingencies

Most answer "if this assumption is wrong" with "I'd change my approach" — no specificity. This tells you nothing about what to actually do differently.

Only **2 students** design real contingencies:
- File 6: two different artifacts depending on which assumption holds
- File 10: severity framework that shifts the intervention type

### Pattern 3: Projection of Own Experience

Students assume partner's internal experience matches their own. Some caught it explicitly:
- *"I have no evidence. I just used my experience with the general dislike of apps from the popular opinion online."*
- *"I think that my solution to push him might be interpretation because that's what I needed to fix a similar issue."*

Most don't catch it at all. The students who named their projection are ahead — not because they avoided it, but because they can see it.

---

## Additional Finding: Dojo Conversations Richer Than Scores Suggest

Dojo conversations show students engaging substantively — asking real questions, responding to AI pushback, working through ideas. But they don't complete the final extraction step: writing down and categorizing their assumptions as a deliverable.

The analysis pipeline scored these as 1/5 because no deliverable was produced. But the conversations show genuine thinking that didn't get captured.

**Implication:** The assignment may need a clearer "stop and write your final list" gate — a hard transition point from conversation mode to deliverable mode. The thinking is happening; the output isn't.